College Football Outcome Predictor
Patrick Sheeran (sheeran.17)
Kunal Arya (arya.36)
Nathan Gray (gray.1273)

Introduction
	Our project is about using statistics provided in a college football game to see how accurate they are in predicting the winner. We were inspired to research this topic after watching many games where different announcers always proclaim that having fewer turnovers will win the game. Our goal for this project was to see how accurately artificial intelligence could predict the winner of a game based off of the statistics alone. We wanted to see how important these stats really were to the outcome of the game, or if they did not really matter.

Data
Our group got our data from the free API at https://collegefootballdata.com/. In order to use the data we created a free account and were emailed a code that was entered into our HTTP requests when we wanted to get the data. We had to issue 224 HTTP requests to the API server because the data was formatted by conference and year. We used 14 conferences for our data points spanning 16 years. Over the span of 16 years with 14 conferences we got a total of 14,680 games to use as data points from this API. The API returned it as JSON format, and from there we converted it into data frames to generate CSV files. The section below discusses more about the splitting of the code, and how we generate the files.

Code Structure
	Our code was created in Google Colab and we made it into two separate files. We felt that this split was necessary to keep the code clean, fast, and easy to run to check the accuracy. The first file that we created is called our data collector. This file is in charge of sending out the HTTP requests to get the data. The data is then returned in JSON format where we then print it out and create a file called data.json. This file is saved directly into google drive in the directory ending in /MyDrive. From there the file is easy to access while signed in and granted permission. From there we read the JSON file in with the goal of converting it to a python pandas data frame. Our goal for this was to make 4 different data frames, which were a training and testing data set for each algorithm. While the data was read in, we have a function in the data collector called compare that compares the two teams and chooses which team has the higher attribute. From there the data frames were replaced with a 1 or 0 for the decision tree, and the perceptron data frames were replaced with the difference of the two values. Once these data frames were created, they were exported as CSV files and ended up in google drive. After this, we uploaded the raw CSV files to github so that we could call the URL in the other section of code to retrieve the files without having to manually select them or upload them everytime we wanted to run the testing file. Our second section of code is our actual algorithms section. This code file reads the CSV files in from the github link. After this it scans through and replaces any missing data based off of which algorithms data frame it is on. After this both algorithms can be started and the accuracy is printed. The data collector code file takes about 20 minutes to complete running while the testing file only takes roughly 2-3 minutes. We felt that splitting up the files allowed us to limit the amount of time between tests of accuracy, help the grader save time, and reduce the traffic to the API server since getting the data again would be redundant. Another benefit to splitting up the code files is for simplicity of running. The grader can simply click the “run all” option in our algorithm file to quickly get both of our accuracies without having to make all the data requests. Links to the two Google Colab files can be found in the readme that is included in the project submission.

Attributes
	The API provides about 24 different attributes for each game recorded. We noticed that about 4 of the columns were missing data for most (roughly 80%) of all the games and decided that this was too infrequent of a stat to be factored into our algorithms. The columns with this much data missing were removed from the data frames and CSV files. These removed attributes were fourth down efficiency, kick return yards, kick return touchdowns, and kick return attempts. Each data point represented a game, and each game had all the attributes for both teams. Our decision column is the score for the game. The complete list of the data that we compared is: possession time, interceptions, fumbles lost, turnovers, total penalty yards, yards per rush attempt, number of rushing attempts, rushing yards, yards per pass, completions percentage, net passing yards, total yards, third down efficiency, first downs, punt returns, punt return touchdowns, punt return yards, fumbles recovered, and which team was the home team. These are the complete list of attributes that we used for our algorithms. For the decision tree data frame we had to make the data categorical. For this we used a 1 if the team had for example more passing yards than the other team. The other team in this example would get a 0 in that same column for that game. For the perceptron algorithm, we needed to keep the data numerical so we used the difference in the stats to fulfill the columns. The team with the lower stats was subtracted from the team with the higher stats and that number was assigned to the column. The decision column for both algorithms was a 1 or a 0 based on if they won or lost the game respectively.

Data replacement
	Occasionally, the data we received was missing a value. In order to ensure that these did not impact our accuracy in any way we had to replace them. In the decision tree’s data frame if there was any data missing it was replaced with the most common value of the column. In the data frame for perceptron the value had to be numerical and so we decided that a mode was likely not going to help because the attribute values would likely be unique. To ensure we got a good replacement value we decided to use the mean of the entire column to replace any missing data for perceptron. We felt that this was the best way to replace the occasional missing data for our algorithms. Like discussed above, any attribute columns that were mostly empty were removed from our data set and not considered by our algorithms.

Splitting the data
	We had 14,680 games over the span of 16 seasons. To split the data into training and testing data we decided to do it every 10 games. The reason that we did not use a chunk of games at the beginning or at the end of the data is that those games would be solely from one era of college football. Since the game strategy evolves over time, we figured that for the best results we had to make sure the training and testing data were composed of the same ratio of older and newer games to give us better accuracy. Every 10th game felt like a good balance to achieve that. By splitting it to every 10th game, this leaves us with 13,212 games for our training data and 1,468 games for our testing data. 

Decision Tree

Algorithm Description

Our first algorithm we decided to implement was the decision tree. In the context of a classification problem, a decision tree will make sequential decisions about the outcome variable based on previous data. More specifically, once the tree has been trained, the model will test certain conditions regarding the input data. If a certain condition is met, then the data will be passed down that “branch” and may be further tested with different conditions. Eventually, the end of the tree will be reached (“leaf node”), which describes the class label it identifies the input as. In the context of our problem setting, this means that when the statistics of a college football game with respect to the two teams playing is passed down the tree, the tree will determine if the first team won or lost (2 total classes).
	To create the tree, the tree requires a splitting criteria at each node. This is selected by calculating the various different entropies of each attribute in the data, and then selecting the attribute with the least entropy (or the most information gain). Entropy can be viewed as a measure of uncertainty, so choosing an attribute with the least entropy will ensure our tree is more accurate. Entropy for one variable is calculated using the equation below, where S represents an attribute, c represents the number of classes an attribute can take, and pi represents the probability that the decision column is a “yes” given the specific class.
Entropy(S) = i=1c-pilog2pi
Entropy with two variables is calculated using the equation below, where S and T represent attributes, c is a class in the domain of attribute T, P(c) represents the probability of the class occurring in the data, and E(c) represents the entropy of that class.
Entropy(S, T) = c  TP(c)E(c)
	If the entropy of an attribute is 0, then that means it is homogenous. This means the tree can give the input a class label. Once an attribute has been selected as the root node for the tree, we continue to build the tree in a recursive structure until homogeneity has been reached for all branches. For our data, “totalYards” was the splitting criteria for our root node, meaning that if the first team had more total yards than the second team, the tree would traverse down a certain path, where more nodes and conditions would be used to identify what path to take to eventually reach a label of 1 or 0 (representing a win or loss for the first team, respectively). 

Performance Report

Our tree was able to achieve an accuracy of 82.83%, meaning that it accurately predicted the outcome of a football game 82.83% of the time. The two attributes at the top of the tree our model created are totalYards and turnovers, suggesting that these two statistics are critical in winning a college football game.
	While 82.83% is great, there is room for further improvement. Our algorithm does not implement a max depth to the tree, so it could be that the tree created could be overfitted to the training-data, and does not perform as well to the testing-data. Implementing a max depth as well as using more data would be a great way to improve accuracy.





Perceptron

Algorithm Description

The second algorithm we decided to implement was the Perceptron algorithm. We decided to go with perceptron because we wanted to be able to use numerical data rather than categorical to see if the magnitude by which teams win or lose each category would do a better job at predicting game outcomes than simply whether or not they won the category. To implement this algorithm, we arranged the data by game, like in the decision tree algorithm, and put the differential for each stat by subtracting the stat for the first team from the stat for the second team listed for that game in the data. The decision column was a 1 if the first team won and a 0 if they lost. We then split off the decision column from the data for both the training and testing data, which was a 90-10 split from the full data set. 
	Once the data is split, we run both the testing and training data through the perceptron algorithm with a max iterations of 100 and a learning rate of 0.01. Inside the algorithm, we initialize all of the weights to 0.005. We then loop for the max number of iterations, and inside that loop, we randomly loop through each data instance and get a prediction for whether team 1 won or lost that game (which is either 1 or 0). To do this, we simply take the data production of the weights with the current data instance, then take the sign of that. We then change any -1s to 0. Finally, if the predicted win value doesn’t match the actual win value, we update the weights by adding the learning rate times the actual win value times the current data instance. We then check the accuracy of both the test and the train data.

Performance Report

When we first created the algorithm, we initialized all of the weights to 0. This 
gave us an accuracy of about 65%. We thought that maybe we were doing something wrong, so we changed the algorithm from multiplying by 0 if the real Y value was 0 to multiplying by -1. This upped our accuracy to about 70%. Professor Tabassum then suggested to us that we change the initialization of the weight values to be a random number between 0 and 1. This upped our accuracy to about 87%. We then became curious about trying out different methods of this, so we tested initializing all of the weights to the same value. We tested 0.1, and got around 90% accuracy. We then tested .01 and got around 85%, and 0.2 gave around the same accuracy, so we settled on 0.1. We then tested our learning rate and max iterations. Decreasing max iterations to 50 sent the accuracy back down to around 86%. Increasing it to 200 made the accuracy around 90% again, and making it 500 made the accuracy back to around 87%, so we kept it at 100. For the learning rate, we started by decreasing it from 0.1 to 0.01. This brought an accuracy of around 87%. We then tried 0.001 and 0.0001, which had accuracies around 84% and 89% respectively% numbers lower than .0001 kept around that 89% mark. A 0.2 learning rate gave an 86% accuracy. So, we finally settled on initializing all weights to 0.1, having a max iterations of 100 and a learning rate of 0.1, and that gave us a training accuracy of 89.8% and a test accuracy of 90.4%. This accuracy is better than that of the decision tree, and we believe that that is because there is some importance in how much you win these categories by. The accuracy only went up by about 8%, so it’s not extremely important, but it definitely seems like there it’s not irrelevant and is actually helpful in predicting the outcomes of these games.

Conclusion
	Based on the accuracies that we got for both our algorithms we can say that typically there are a few statistics that do in fact have a large impact on who wins the game. We found that these attributes are typically total yards and turnovers. We found a very high correlation between winning these categories and winning the game. This helped us reach a conclusion on our original goal that these stats are significant in deciding who wins the games and that the announcers are right. It also seems to be the case that the amount of difference in the statistics also plays a significant role in whether a team wins a game, since the perceptron algorithm performed better than the decision tree.
	For future studies, it will be interesting to look at how a Neural Network with multiple hidden layers would perform. Seeing how the Perceptron layer performed quite well, a Neural Network would seem to be promising in predicting outcomes. 

