{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "deJ-WVQHdjAN"
      },
      "source": [
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_CSL3l6AsfW"
      },
      "source": [
        "# Node class representing nodes in the decision tree\n",
        "class Node:\n",
        "  def __init__(self, node_type, attr, value):\n",
        "    self.node_type = node_type\n",
        "    self.attr = attr\n",
        "    self.value = value\n",
        "    self.children = []\n",
        "\n",
        "  def append_child(self, child):\n",
        "    self.children.append(child)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "athheV-gAy3q"
      },
      "source": [
        "# Entropy function to calculate the entropy between the 2 given values\n",
        "def entropy_2vals(e, p):\n",
        "  if e == 0 or p == 0: return 0\n",
        "  sum = float(e+p)\n",
        "  return -1*(e/sum*math.log(e/sum, 2)) - (p/sum*math.log(p/sum, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4MUzZUaA1kT"
      },
      "source": [
        "# Gets entropies for given working set\n",
        "def entropy(working_set):\n",
        "  # Calculate all entropy values\n",
        "  entropies = {}\n",
        "  for col in working_set.columns:\n",
        "    if col == 'points': continue\n",
        "\n",
        "    unique_attr = working_set[col].unique()\n",
        "    freq_table = pd.DataFrame(data=np.zeros((2,len(unique_attr)), dtype=np.int64), index=[0,1], columns=unique_attr)\n",
        "\n",
        "    for row in working_set[col].index:\n",
        "      attr = working_set[col][row]\n",
        "      freq_table[attr][df_train['points'][row]] += 1\n",
        "\n",
        "    entro = 0\n",
        "    if len(working_set.index) > 0:\n",
        "      for freq_col in freq_table.columns:\n",
        "        entro += (freq_table[freq_col].sum()/len(working_set.index)) * entropy_2vals(freq_table[freq_col][0], freq_table[freq_col][1])\n",
        "\n",
        "    entropies[col] = entro\n",
        "\n",
        "  return entropies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0geMqWDpA5ka"
      },
      "source": [
        "# Gets entropy of e column\n",
        "def class_entropy(rows):\n",
        "  # Get working set of rows\n",
        "  working_set = df_train.iloc[rows, [0]]\n",
        "\n",
        "  # Calculate entropy for classes\n",
        "  value_counts = working_set.value_counts().values\n",
        "  if len(value_counts) < 2:\n",
        "    return 0\n",
        "  else:\n",
        "    return entropy_2vals(working_set.value_counts().values[0], working_set.value_counts().values[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ej7ccc5A8P5"
      },
      "source": [
        "# Recursive function to make the decision tree\n",
        "def make_tree(learning_set):\n",
        "  # Get class entropy given attribute values\n",
        "  class_entro = class_entropy(learning_set.index)\n",
        "\n",
        "  # Get entropies\n",
        "  entropies = entropy(learning_set)\n",
        "\n",
        "  # Check if homogenous group or not and respond appropriately\n",
        "  if class_entro == 0:\n",
        "    leaf_node = Node(node_type='leaf', attr=None, value=df_train['points'][learning_set.index[0]])\n",
        "    return leaf_node\n",
        "  else:\n",
        "    # Calculate the information gains\n",
        "    info_gains = {}\n",
        "    for entro in entropies:\n",
        "      info_gains[entro] = class_entro - entropies[entro]\n",
        "\n",
        "    if len(learning_set.columns) == len(df_train.columns) - 1:\n",
        "      print(\"Information gains for iteration 1:\")\n",
        "      print(info_gains)\n",
        "\n",
        "    root_col = max(info_gains, key=info_gains.get)\n",
        "    root = Node(node_type='decision', attr=root_col, value=None)\n",
        "\n",
        "    for attr in learning_set[root_col].unique():\n",
        "      attr_node = Node(node_type='attr', attr=root_col, value=attr)\n",
        "      new_columns = np.delete(learning_set.columns.values, np.where(learning_set.columns.values == root_col))\n",
        "      new_learning_set = learning_set[learning_set[root_col]==attr]\n",
        "      new_learning_set = new_learning_set.loc[:, new_columns]\n",
        "      attr_node.append_child(make_tree(new_learning_set))\n",
        "      root.append_child(attr_node)\n",
        "\n",
        "    return root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S2UFMG1A-8l"
      },
      "source": [
        "# Returns the predicted value for the given rowe in the test_data_frame\n",
        "def traverse_tree(decision_tree, row):\n",
        "  if decision_tree.node_type == 'leaf':\n",
        "    return decision_tree.value\n",
        "  elif decision_tree.node_type == 'attr':\n",
        "    return traverse_tree(decision_tree.children[0], row)\n",
        "  else:\n",
        "    for child in decision_tree.children:\n",
        "      if child.value == df_test[child.attr][row]:\n",
        "        return traverse_tree(child, row)\n",
        "        break;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_i2TeA2dd1m"
      },
      "source": [
        "# Prints the tree (for testing purposes)\n",
        "def print_tree(the_tree):\n",
        "  print('(', end='')\n",
        "  print(the_tree.node_type, end='')\n",
        "  print(str(the_tree.attr), end='')\n",
        "  print(str(the_tree.value), end='')\n",
        "  for child in the_tree.children:\n",
        "    print_tree(child)\n",
        "  print(')', end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtEQlyUwkDLk"
      },
      "source": [
        "# Get the raw data\n",
        "train_data_url = 'https://raw.githubusercontent.com/ptsheeran/AI/main/TrainingData.csv'\n",
        "test_data_url = 'https://raw.githubusercontent.com/ptsheeran/AI/main/TestingData.csv'\n",
        "\n",
        "# Get a data frame for the train data\n",
        "df_train = pd.read_csv(train_data_url)\n",
        "df_test = pd.read_csv(test_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbUrXngePclW"
      },
      "source": [
        "# Replace unknown values\n",
        "for col in df_train.columns:\n",
        "  for row in df_train[col].index:\n",
        "    if df_train[col][row] == '?':\n",
        "      mode = df_train[col].mode().values[0]\n",
        "      df_train[col][row] = mode\n",
        "\n",
        "for col in df_test.columns:\n",
        "  for row in df_test[col].index:\n",
        "    if df_test[col][row] == '?':\n",
        "      mode = df_test[col].mode().values[0]\n",
        "      df_test[col][row] = mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL_sX1ifJ3Ys",
        "outputId": "aa0b365a-04fc-4c39-a0ce-fe60d42a7795"
      },
      "source": [
        "# Start with all the attributes, except decision column\n",
        "starting_attributes = df_train.columns[:len(df_train.columns)-2]\n",
        "\n",
        "# Make the starting learning set, which is everything except the first column\n",
        "starting_learning_set = df_train.loc[:,starting_attributes]\n",
        "\n",
        "# Make the decision tree\n",
        "tree = make_tree(starting_learning_set)\n",
        "\n",
        "# Testing the tree\n",
        "total_correct = 0\n",
        "for row in df_test.index:\n",
        "  if traverse_tree(tree, row) == df_test['points'][row]:\n",
        "    total_correct += 1\n",
        "\n",
        "print(\"Decision Tree Accuracy: \" + str((total_correct/len(df_test.index))*100) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree Accuracy: 82.83378746594006%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYboWGWBrvva"
      },
      "source": [
        "def Perceptron(X, Y, learningRate=0.01, maxIter=100):\n",
        "    np.random.seed(1)\n",
        "    N = X.shape[1]\n",
        "    D_plus_1 = X.shape[0]\n",
        "    w = np.zeros((D_plus_1, 1))\n",
        "    w += .1\n",
        "\n",
        "    for t in range(maxIter):\n",
        "        permutation = np.random.permutation(N)\n",
        "        X = X[:, permutation]\n",
        "        Y = Y[permutation, :] \n",
        "        for n in range(N):\n",
        "            y1 = np.sign(np.dot(np.transpose(w), X[:, n]))\n",
        "            if (y1 == -1):\n",
        "              y1 += 1\n",
        "            if (y1 != Y[n, :]):\n",
        "              if (Y[n, :] == 0):\n",
        "                w += learningRate * -1 * X[:, n].reshape(-1,1)\n",
        "              else:\n",
        "                w += learningRate * np.transpose((np.dot(Y[n, :].reshape(-1,1), np.transpose(X[:, n].reshape(-1,1)))))\n",
        "\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHhRzSPBf1ag"
      },
      "source": [
        "def Accuracy(X, Y, w):\n",
        "  Y_hat = np.sign(np.matmul(X.transpose(), w))\n",
        "  Y_hat = np.where(Y_hat == -1, 0, Y_hat)\n",
        "  correct = (Y_hat == Y)\n",
        "  return math.fsum(correct) / len(correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYRfIyujXiXi"
      },
      "source": [
        "#Start of logistic regression\n",
        "import pandas as pd\n",
        "log_train_data_url = 'https://raw.githubusercontent.com/ptsheeran/AI/main/LogTrainingData.csv'\n",
        "log_test_data_url = 'https://raw.githubusercontent.com/ptsheeran/AI/main/LogTestingData.csv'\n",
        "#Get the data CSV files we created from github for simplicity\n",
        "log_df_train = pd.read_csv(log_train_data_url)\n",
        "log_df_test = pd.read_csv(log_test_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etRPsMMVfZJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19aecf49-080f-4168-8e08-95d4a57188b5"
      },
      "source": [
        "import pandas as pd\n",
        "#find the mean\n",
        "testValues = 0\n",
        "trainValues = 0\n",
        "avgTest = []\n",
        "avgTrain = []\n",
        "sum = 0.0\n",
        "for col in log_df_test.columns:\n",
        "  for row in log_df_test[col].index:\n",
        "    if log_df_test[col][row] != '?':\n",
        "      #print(row, sum,testValues,log_df_test[col][row])\n",
        "      sum += float(log_df_test[col][row])\n",
        "      testValues +=1\n",
        "      \n",
        "    \n",
        "  sum = sum / testValues \n",
        "  avgTest.append(sum)\n",
        "  sum = 0.0\n",
        "  testValues = 0\n",
        "\n",
        "sum = 0.0\n",
        "#find the mean of the train columns\n",
        "for col in log_df_train.columns:\n",
        "  for row in log_df_train[col].index:\n",
        "    if log_df_train[col][row] != '?':\n",
        "      sum += float(log_df_train[col][row])\n",
        "      trainValues +=1\n",
        "  sum = sum / trainValues \n",
        "  avgTrain.append(sum)\n",
        "  sum = 0.0\n",
        "  trainValues = 0\n",
        "\n",
        "# Replace unknown values\n",
        "colNum = 0\n",
        "for col in log_df_train.columns:\n",
        "  for row in log_df_train[col].index:\n",
        "    if log_df_train[col][row] == '?':\n",
        "      log_df_train[col][row] = avgTrain[colNum]\n",
        "  colNum +=1\n",
        "\n",
        "colNum = 0\n",
        "for col in log_df_test.columns:\n",
        "  for row in log_df_test[col].index:\n",
        "    if log_df_test[col][row] == '?':\n",
        "      log_df_test[col][row] = avgTest[colNum]\n",
        "  colNum +=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6gy5UfAf83I",
        "outputId": "9b17afa4-2616-44ba-c5ef-d6a445d31564"
      },
      "source": [
        "# Start with all the attributes, except decision column\n",
        "working_attributes = log_df_train.columns[:len(log_df_train.columns)-2]\n",
        "\n",
        "X_train = np.asarray(log_df_train.loc[:, working_attributes].T, dtype=np.float64)\n",
        "Y_train = np.asarray(log_df_train['points'], dtype=np.float64).reshape(len(log_df_train.index), 1)\n",
        "\n",
        "X_test = np.asarray(log_df_test.loc[:, working_attributes].T, dtype=np.float64)\n",
        "Y_test = np.asarray(log_df_test['points'], dtype=np.float64).reshape(len(log_df_test.index), 1)\n",
        "\n",
        "w = Perceptron(X_train, Y_train,  maxIter=100, learningRate=.1)\n",
        "\n",
        "training_accuracy = Accuracy(X_train, Y_train, w)\n",
        "test_accuracy = Accuracy(X_test, Y_test, w)\n",
        "print(\"Perceptron Accuracy: training set: \", training_accuracy)\n",
        "print(\"Perceptron Accuracy: test set: \", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perceptron Accuracy: training set:  0.8977444747199516\n",
            "Perceptron Accuracy: test set:  0.9046321525885559\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}